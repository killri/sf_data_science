{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оптимизации можно разделить на 3 группы:\n",
    "\n",
    "* методы нулевого порядка (их работа основана на оценке значений самой целевой функции в разных точках);\n",
    "* методы первого порядка (при работе они используют первые производные в дополнение к информации о значении функции);\n",
    "* методы второго порядка (для них необходимо оценивать и значение функции, и значение градиента, и гессиан (матрицу Гессе)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВАРИАЦИИ ГРАДИЕНТНОГО СПУСКА**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Основные проблемы при реализации градиентного спуска:*\n",
    "\n",
    "* Классический градиентный спуск склонен застревать в точках локального минимума и даже в седловых точках, словом — везде, где градиент равен нулю. Это мешает найти глобальный минимум.\n",
    "* Обычно у оптимизируемой функции очень сложный ландшафт: где-то она совсем пологая, где-то более крутой обрыв. В таких ситуациях градиентный спуск показывает не лучшие результаты. Так происходит потому, что в алгоритме градиентного спуска фиксированный шаг, а нам в идеале хотелось бы его изменять в зависимости от формы функции прямо в процессе обучения.\n",
    "* Много проблем возникает из-за темпа обучения: при низком алгоритм сходится невероятно медленно, при быстром — «пролетает» мимо минимумов.\n",
    "* При обучении градиентного спуска координаты в некоторых измерениях могут резко изменяться, что приводит к плохой обобщающей способности алгоритма. Можно попытаться придать каждого признаку бόльшую важность, но в таком случае есть серьёзный риск переобучить модель."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Три основных вариации градиентного спуска:*\n",
    "\n",
    "* Batch Gradient Descent;\n",
    "* Stochastic Gradient Descent;\n",
    "* Mini-batch Gradient Descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BATCH GRADIENT DESCENT (пакентны/ванильный градиентный спуск)**\n",
    "\n",
    "По сути это и есть классический (ванильный) градиентный спуск"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой градиентный спуск достаточно хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки. Такие, к примеру, мы можем наблюдать у линейной или логистической регрессии. Мы обсуждали, что градиентный спуск не умеет находить глобальный минимум среди прочих, но, например, для выпуклой функции он может это сделать. Поэтому с выпуклыми функциями (допустим, со среднеквадратичной ошибкой для линейной регрессии) нам удобнее всего использовать именно его."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но всё усложняется, когда мы хотим применить его при обучении нейронных сетей. Как уже говорилось, у таких моделей функция потерь имеет много локальных экстремумов, в каждом из которых градиентный спуск может застрять. Когда данных очень много, а оптимизируемая функция очень сложная, данный алгоритм применять затруднительно, так как поиск градиента по всем наблюдениям делает задачу очень затратной в плане вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOCHASTIC GRADIENT DESCENT (стохастический градиентный спуск)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стохастический градиентный спуск очень часто используется в нейронных сетях и сокращает время машинных вычислений, одновременно повышая сложность и производительность крупномасштабных задач."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINI-BATCH GRADIENT DESCENT (мини-пакетныq градиентныq спуск)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент это наиболее популярная реализация градиентного спуска, которая используется в глубоком обучении (т. е. в обучении нейронных сетей)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе обучения модели с помощью мини-пакетного градиентного спуска обучающая выборка разбивается на пакеты (батчи), для которых рассчитывается ошибка модели и пересчитываются веса."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, с одной стороны, мы используем все преимущества обычного градиентного спуска, а с другой — уменьшаем сложность вычислений и повышаем их скорость по аналогии со стохастическим спуском. Кроме того, алгоритм работает ещё быстрее за счёт возможности применения векторизованных вычислений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\\right)$ т.е. мы применияем обычный спучк, но к части данных для n наблюдений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 2.7\n",
    "#Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии. \n",
    "# Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки.\n",
    "\n",
    "#Загрузите стандартный датасет об алмазах из библиотеки Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"price\")\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33), значение random_state должно быть равно 42.\n",
    "\n",
    "Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском (класс SGDRegressor). Отберите с помощью GridSearchCV оптимальные параметры по следующей сетке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = SGDRegressor()\n",
    "\n",
    "grid = {\"loss\": [\"squared_error\", \"epsilon_insensitive\"],\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"alpha\": np.logspace(-3, 3, 15),\n",
    "    \"l1_ratio\": np.linspace(0, 1, 11),\n",
    "    \"max_iter\": np.logspace(0, 3, 10).astype(int),\n",
    "    \"random_state\": [42],\n",
    "    \"learning_rate\": [\"constant\"],\n",
    "    \"eta0\": np.logspace(-4, -1, 4)}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=grid, \n",
    "    n_jobs = -1\n",
    ")  \n",
    "#grid_search.fit(X_train, y_train) \n",
    "#y_test_pred = grid_search.predict(X_test)\n",
    "#mean_squared_error(y_test_pred, y_test)\n",
    "#0.04349853776551417\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**АЛГОРИТМЫ, ОСНОВАННЫЕ НА ГРАДИЕНТНОМ СПУСКЕ**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда в данных присутствуют очень редко встречающиеся входные параметры.\n",
    "\n",
    "Например, если мы классифицируем письма на «Спам» и «Не спам», таким параметром может быть очень специфическое слово, которое встречается в спаме намного реже других слов-индикаторов. Или, если мы говорим о распознавании изображений, это может быть какая-то очень редкая характеристика объекта.\n",
    "\n",
    "В таком случае нам хотелось бы иметь для каждого параметра свою скорость обучения: чтобы для часто встречающихся она была низкой (для более точной настройки), а для совсем редких — высокой (это повысит скорость сходимости). То есть нам очень важно уметь обновлять параметры модели, учитывая то, насколько типичные и значимые признаки они кодируют.\n",
    "\n",
    "Решение этой задачи предложено в рамках алгоритма AdaGrad (его название обозначает, что это адаптированный градиентный спуск). В нём обновления происходят по следующему принципу:\n",
    "\n",
    "$G_t = G_t + g^2_t$\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы храним сумму квадратов градиентов для каждого параметра. Таким образом, параметры, которые сильно обновляются каждый раз, начинают обновляться слабее. Скорость обучения в таком алгоритме будет постоянно затухать. Мы будем начинать с больших шагов, и с приближением к точке минимума шаги будут уменьшаться — это улучшит скорость сходимости.\n",
    "\n",
    "Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако снижение скорости обучения в AdaGrad иногда происходит слишком радикально, и она практически обнуляется. Чтобы решить эту проблему, были созданы алгоритмы RMSProp, AdaDelta, Adam и некоторые другие."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**МЕТОД НЬЮТОНА**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея такая. Решаем уравнение f(x)=0. x0 - какая-то начальная точка. \n",
    "Строим касательную в x0 и считаем, где она пересечет ось x. Это точка будет x1. И т.д. Алгоритм заканчивается когда мы получили x(n) = x(n+1) с требуемой точностью\n",
    "\n",
    "$x_{n+1} = x_n - f(x_n)/f'(x_n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 3.1\n",
    "#Найдите третий корень полинома\n",
    "#f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2\n",
    "#взяв за точку старта 0.7. Введите получившееся значение с точностью до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 6*x**5 - 5*x**4 - 4*x**3 + 3*x**2\n",
    "\n",
    "def df(x):\n",
    "    return 30*x**4 - 20*x**3 - 12*x**2 + 6*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zero(f,df,x0,acc):\n",
    "    x_old = x0\n",
    "    while True:\n",
    "        x_new = x_old - f(x_old)/df(x_old)\n",
    "        if round(x_new,acc) == round(x_old,acc):\n",
    "            return round(x_old,acc)\n",
    "        x_old = x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_zero(f,df,0.7,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, если мы будем решать $f'(x) = 0$, то сможем найти точки подозрительные на экстремум.\n",
    "\n",
    "А многомерном случае это выглядит так:\n",
    "\n",
    "$x^{(n+1)} = x^{(n)} - \\left [Hf(x^{(n)})  \\right ]^{-1} \\nabla f(x^{(n)})$\n",
    "\n",
    "где $Hf(x^{(n)})$ - значение Геccиана (матрицы из вторых производных) в $x^{(n)}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что эта формула совпадает с формулой для градиентного спуска, но вместо умножения на learning rate (темп обучения) используется умножение на обратную матрицу к гессиану. Благодаря этому функция может сходиться за меньшее количество итераций, так как мы учитываем информацию о выпуклости функции через гессиан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.695121951219512\n",
      "11.734125501243229\n",
      "7.1123493600499685\n",
      "5.365000391507974\n",
      "5.015260627016227\n",
      "5.000029000201801\n",
      "5.000000000105126\n",
      "5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "initial_value = 42\n",
    "iter_count = 0\n",
    "x_curr = initial_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newtons_method(f, der, eps, init):\n",
    "    iter_count = 0\n",
    "    x_curr = init\n",
    "    f = f(x_curr)\n",
    "    while (abs(f) > eps):\n",
    "        f = f(x_curr)\n",
    "        f_der = der(x_curr)\n",
    "        x_curr = x_curr - (f)/(f_prime)\n",
    "        iter_count += 1\n",
    "    return x_curr\n",
    " \n",
    "from scipy.optimize import newton\n",
    "newton(func=func1,x0=50,fprime=func2, tol=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Плюсы метода Ньютона*\n",
    "\n",
    "* Если мы минимизируем квадратичную функцию, то с помощью метода Ньютона можно попасть в минимум целевой функции за один шаг.\n",
    "* Также этот алгоритм сходится за один шаг, если в качестве минимизируемой функции выступает функция из класса поверхностей вращения (т. е. такая, у которой есть симметрия).\n",
    "* Для несимметричной функции метод не может обеспечить сходимость, однако скорость сходимости  всё равно превышает скорость методов, основанных на градиентном спуске.\n",
    "\n",
    "*Минусы метода Ньютона*\n",
    "\n",
    "* Этот метод очень чувствителен к изначальным условиям.\n",
    "\n",
    "При использовании градиентного спуска мы всегда гарантированно движемся по антиградиенту в сторону минимума. В методе Ньютона происходит подгонка параболоида к локальной кривизне, и затем алгоритм движется к неподвижной точке данного параболоида. Из-за этого мы можем попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций седловые точки встречаются намного чаще экстремумов.\n",
    "\n",
    "Поэтому здесь необходимо обозначить ограничение: метод Ньютона стоит применять только для задач, в которых целевая функция выпуклая.\n",
    "\n",
    "Впрочем, это не является проблемой. В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или логистической регрессии мы как раз ищем минимум у выпуклой целевой функции, то есть данный алгоритм подходит нам во многих случаях.\n",
    "\n",
    "* Также метод Ньютона может быть затратным с точки зрения вычислительной сложности, так как требует вычисления не только градиента, но и гессиана и обратного гессиана (при делении на матрицу необходимо искать обратную матрицу).\n",
    "\n",
    "Если у задачи много параметров, то расходы на память и время вычислений становятся астрономическими. Например, при наличии 50 параметров нужно вычислять более 1000 значений на каждом шаге, а затем предстоит ещё более 500 операций нахождения обратной матрицы. Однако метод всё равно используют, так как выгода от быстрой сходимости перевешивает затраты на вычисления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 3.6\n",
    "\n",
    "#Дана функция f(x) = x^3 - 72x - 220. Найдите корень в окрестностях точки x0 = 12. \n",
    "#Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.727"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f3(x):\n",
    "    return x**3 - 72*x - 220\n",
    "def df3(x):\n",
    "    return 3*x**2 - 72\n",
    "\n",
    "find_zero(f3,df3,12,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 3.7\n",
    "#Найдите положительный корень для уравнения x^2 + 9x - 5 = 0\n",
    "#В качестве стартовой точки возьмите x0=2.2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f4(x):\n",
    "    return x**2 + 9*x - 5\n",
    "def df4(x):\n",
    "    return 2*x+9\n",
    "\n",
    "find_zero(f4,df4,2.2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 3.9\n",
    "\n",
    "#С помощью метода Ньютона найдите точку минимума для функции f(x) = 8x^3 - 2x^2 - 450\n",
    "#В качестве стартовой точки возьмите 42, точность примите за 0.0001\n",
    "#Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1667"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f5(x):\n",
    "    return 24*x**2 - 4*x\n",
    "def df5(x):\n",
    "    return 48*x-4\n",
    "\n",
    "find_zero(f5,df5,42,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Квазиньютоновские методы**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы использует найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{k+1} = x_k - H_k \\nabla f(x_k)$\n",
    "\n",
    "В данном случае вместо обратного гессиана появляется матрица $H_k$, которая строится таким образом, чтобы максимально точно аппроксимировать настоящий обратный гессиан.\n",
    "\n",
    "$H_k - \\left [\\nabla^2 f(x_k) \\right ]^{-1} \\to 0 \\ при \\ k \\to \\infty$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта матрица обновляется на каждом шаге, и для этого существуют разные способы. Для каждого из способов есть своя модификация квазиньютоновского метода. Эти способы объединены ограничением: процесс обновления матрицы должен быть достаточно эффективным и не должен требовать вычислений гессиана. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три самые популярные схемы аппроксимации:\n",
    "\n",
    "* симметричная коррекция ранга 1 (SR1);\n",
    "* схема Дэвидона — Флетчера — Пауэлла (DFP);\n",
    "* схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [1, 1]\n",
    "# реализуем алгоритм L-BFGS-B\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 4.1\n",
    "\n",
    "#Найдите точку минимума для функции f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20.\n",
    "#В качестве стартовой возьмите точку (-400,-400).\n",
    "#Значения координат округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 11\n",
      "Решение: f([-4.  1.]) = -1.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**2.0 - x[0]*x[1]+ x[1]**2.0 + 9*x[0] - 6*x[1] + 20\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - x[1] + 9, -x[0] + 2* x[1] - 6])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [-400, -400]\n",
    "# реализуем алгоритм \n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 4.4\n",
    "#Найдите минимум функции f(x) = x**2 - 3x + 45 с помощью квазиньютоновского метода BFGS.\n",
    "#В качестве стартовой точки возьмите x = 0.\n",
    "#В качестве ответа введите минимальное значение функции в достигнутой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x**2-3*x+45\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return 2*x-3\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = 0\n",
    "\n",
    "# реализуем алгоритм \n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "#Задание 4.5\n",
    "#Решите предыдущую задачу, применяя модификацию L-BFGS-B.\n",
    "#В каком случае получилось меньше итераций?\n",
    "\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 4.7\n",
    "#Найдите минимум функции f(x,y) = x^4 + 6*y^2 + 10, взяв за стартовую точку (100,100).\n",
    "#Какой алгоритм сошелся быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS\n",
      "Статус оптимизации Optimization terminated successfully.\n",
      "Количество оценок: 37\n",
      "Решение: f([1.31617159e-02 6.65344582e-14]) = 10.00000\n",
      "L-BFGS-B\n",
      "Статус оптимизации CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Количество оценок: 40\n",
      "Решение: f([-9.52718297e-03 -2.32170510e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "# определяем нашу функцию\n",
    "def func(x):\n",
    "    return x[0]**4.0 + 6*x[1]**2 +10\n",
    " \n",
    "#  определяем градиент функции\n",
    "def grad_func(x):\n",
    "    return np.array([4*x[0]**3, 12*x[1]])\n",
    " \n",
    "# определяем начальную точку\n",
    "x_0 = [100, 100]\n",
    "# реализуем алгоритм \n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('BFGS')\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    "\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('L-BFGS-B')\n",
    "print('Статус оптимизации %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейное программирование**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Линейное программирование* — это метод оптимизации для системы линейных ограничений и линейной целевой функции. Целевая функция определяет оптимизируемую величину, и цель линейного программирования состоит в том, чтобы найти значения переменных, которые максимизируют или минимизируют целевую функцию."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача линейного программирования — это задача оптимизации, в которой целевая функция и функции-ограничения линейны, а все переменные неотрицательны."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Целочисленным линейным программированием (ЦЛП)** называется вариация задачи линейного программирования, когда все переменные — целые числа."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В языке Python есть множество библиотек, с помощью которых можно решить задачу линейного программирования. Вот основные, которые мы рассмотрим в данном юните:\n",
    "\n",
    "* SciPy (scipy.optimize.linprog);\n",
    "* CVXPY; \n",
    "* PuLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейное программирование в SciPy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Задача*\n",
    "\n",
    "У нас есть 6 товаров с заданными ценами на них и заданной массой.\n",
    "Вместимость сумки, в которую мы можем положить товары, заранее известна и равна 15 кг.\n",
    "Какой товар и в каком объёме необходимо взять, чтобы сумма всех цен товаров была максимальной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [4, 2, 1, 7, 3, 6] #стоимости товаров\n",
    "weights = [5, 9, 8, 2, 6, 5] #вес товаров\n",
    "C = 15 #вместимость сумки\n",
    "n = 6 #количество товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = - np.array(values) #изменяем знак, чтобы перейти от задачи максимизации к задаче минимизации\n",
    "A = np.array(weights)  #конвертируем список с весами в массив\n",
    "A = np.expand_dims(A, 0) #преобразуем размерность массива\n",
    "b = np.array([C]) #конвертируем вместимость в массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           con: array([], dtype=float64)\n",
       " crossover_nit: 0\n",
       "         eqlin:  marginals: array([], dtype=float64)\n",
       "  residual: array([], dtype=float64)\n",
       "           fun: -52.5\n",
       "       ineqlin:  marginals: array([-3.5])\n",
       "  residual: array([0.])\n",
       "         lower:  marginals: array([13.5, 29.5, 27. ,  0. , 18. , 11.5])\n",
       "  residual: array([0. , 0. , 0. , 7.5, 0. , 0. ])\n",
       "       message: 'Optimization terminated successfully. (HiGHS Status 7: Optimal)'\n",
       "           nit: 0\n",
       "         slack: array([0.])\n",
       "        status: 0\n",
       "       success: True\n",
       "         upper:  marginals: array([0., 0., 0., 0., 0., 0.])\n",
       "  residual: array([inf, inf, inf, inf, inf, inf])\n",
       "             x: array([0. , 0. , 0. , 7.5, 0. , 0. ])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import linprog\n",
    "linprog(c=c, A_ub=A, b_ub=b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейное программирование в CVXPY**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова решим задачу из примера № 1, но уже предположим, что товары нельзя дробить, и будем решать задачу целочисленного линейного программирования.\n",
    "\n",
    "SciPy не умеет решать такие задачи, поэтому будем использовать новую библиотеку CVXPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cvxpy.Variable(shape=n, integer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\killr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "constraint = (A @ x <= b)\n",
    "total_value = c * x\n",
    "x_positive = (x >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[constraint, x_positive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-49.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -0.,  7., -0.,  0.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\killr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cvxpy\\expressions\\expression.py:612: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x = cvxpy.Variable(shape = n, boolean = True)\n",
    "constraint = (A @ x <= b)\n",
    "total_value = c * x\n",
    "x_positive = (x >= 0)\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[constraint, x_positive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что, используя SciPy, мы могли не указывать явно, что  только положительные, так как в линейном программировании считаются только неотрицательные .\n",
    "\n",
    "А вот CVXPY универсальна. Мы просто задали функцию, не указывая, что это линейное программирование. CVXPY «поняла», что это задача оптимизации, и использовала нужные алгоритмы. Поэтому здесь ограничение на положительные  мы указывали явно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейное программирование в PuLP**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Задача*\n",
    "\n",
    "В нашей каршеринговой компании две модели автомобилей: модель A и модель B. Автомобиль A даёт прибыль в размере 20 тысяч в месяц, а автомобиль B — 45 тысяч в месяц. Мы хотим заказать на заводе новые автомобили и максимизировать прибыль. Однако на производство и ввод в эксплуатацию автомобилей понадобится время:\n",
    "\n",
    "* Проектировщику требуется 4 дня, чтобы подготовить документы для производства каждого автомобиля типа A, и 5 дней — для каждого автомобиля типа B.\n",
    "* Заводу требуется 3 дня, чтобы изготовить модель A, и 6 дней, чтобы изготовить модель B.\n",
    "* Менеджеру требуется 2 дня, чтобы ввести в эксплуатацию в компании автомобиль A, и 7 дней —  автомобиль B.\n",
    "* Каждый специалист может работать суммарно 30 дней.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\killr\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pulp\\pulp.py:1352: UserWarning: Spaces are not permitted in the name. Converted to '_'\n",
      "  warnings.warn(\"Spaces are not permitted in the name. Converted to '_'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1.0 B: 4.0 Доход: 200000.0\n"
     ]
    }
   ],
   "source": [
    "from pulp import *\n",
    "problem = LpProblem('Выбор моделе авто для каршеринга', LpMaximize)\n",
    "A = LpVariable('Модель А', lowBound=0, cat=LpInteger)\n",
    "B = LpVariable('Модель B', lowBound=0, cat=LpInteger)\n",
    "\n",
    "problem += 20000*A+45000*B\n",
    "problem += 4*A + 5*B <= 30\n",
    "problem += 3*A + 6*B <= 30\n",
    "problem += 2*A + 7*B <= 30\n",
    "\n",
    "problem.solve()\n",
    "\n",
    "print('A:', A.varValue, 'B:', B.varValue, 'Доход:', value(problem.objective))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 6.1\n",
    "\n",
    "Составьте оптимальный план перевозок со склада № 1 и склада № 2 в три торговых центра с учётом тарифов, запасов на складах и потребностей торговых центров, которые указаны в таблице:\n",
    "\n",
    "img\n",
    "\n",
    "Сформулируйте предложенную задачу как задачу линейного программирования и решите её любым способом (желательно программным).\n",
    "\n",
    "В качестве ответа введите минимальную суммарную стоимость поставки. Ответ округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = cvxpy.Variable(shape=(2,3), integer = True)\n",
    "total_value = x[0,:]@np.array([2,5,3]) + x[1,:]@np.array([7,7,6])\n",
    "con1 = (x[0,:]@np.ones(3) <= 180)\n",
    "con2 = (x[1,:]@np.ones(3) <= 220)\n",
    "con3 = (x[:,0]@np.ones(2) == 110)\n",
    "con4 = (x[:,1]@np.ones(2) == 150)\n",
    "con5 = (x[:,2]@np.ones(2) == 140)\n",
    "con6 = (x >= 0)\n",
    "\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[con1,con2,con3,con4,con5,con6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110.,   0.,  70.],\n",
       "       [ -0., 150.,  70.]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 6.2\n",
    "\n",
    "В прошлом юните мы обсуждали задачу о назначениях исполнителей задач - теперь пришло время решить её.\n",
    "\n",
    "Напомним суть: необходимо распределить пять задач между пятью исполнителями таким образом, чтобы суммарные затраты на работы были наименьшими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5182"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [1000,12,10,19,8],\n",
    "    [12,1000,3,7,2],\n",
    "    [10,3,1000,6,20],\n",
    "    [19,7,6,1000,4],\n",
    "    [8,2,20,4,1000]])\n",
    "\n",
    "B = np.ones((5,5))\n",
    "A*(B+B)\n",
    "A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1000,12,10,19,8],\n",
    "    [12,1000,3,7,2],\n",
    "    [10,3,1000,6,20],\n",
    "    [19,7,6,1000,4],\n",
    "    [8,2,20,4,1000]])\n",
    "\n",
    "x = cvxpy.Variable(shape=(5,5), integer = True)\n",
    "total_value = x[0,:]@A[0,:] + x[1,:]@A[1,:] + x[2,:]@A[2,:] + x[3,:]@A[3,:] + x[4,:]@A[4,:] #не хотел он принмать выражение типа (np.array(x*A)).sum()\n",
    "\n",
    "con1 = (x >= 0)\n",
    "con2 = (x <= 1)\n",
    "\n",
    "con3 = (x[0,:]@np.ones(5) == 1)\n",
    "con2 = (x[1,:]@np.ones(5) == 1)\n",
    "con3 = (x[2,:]@np.ones(5) == 1)\n",
    "con4 = (x[3,:]@np.ones(5) == 1)\n",
    "con5 = (x[4,:]@np.ones(5)== 1)\n",
    "\n",
    "con6 = (x[:,0]@np.ones(5) == 1)\n",
    "con7 = (x[:,1]@np.ones(5) == 1)\n",
    "con8 = (x[:,2]@np.ones(5) == 1)\n",
    "con9 = (x[:,3]@np.ones(5) == 1)\n",
    "con10 = (x[:,4]@np.ones(5)== 1)\n",
    "\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[con1,con2,con3,con4,con5,con6,con7,con8,con9,con10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 6.3\n",
    "\n",
    "Найдите кратчайший маршрут из точки A, который проходит через все другие точки и возвращается в A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1000,10,8,19,12],\n",
    "    [10,1000,20,6,3],\n",
    "    [8,20,1000,4,2],\n",
    "    [19,6,4,1000,7],\n",
    "    [12,3,2,7,1000]])\n",
    "\n",
    "x = cvxpy.Variable(shape=(5,5), integer = True)\n",
    "total_value = x[0,:]@A[0,:] + x[1,:]@A[1,:] + x[2,:]@A[2,:] + x[3,:]@A[3,:] + x[4,:]@A[4,:] #не хотел он принмать выражение типа (np.array(x*A)).sum()\n",
    "\n",
    "con1 = (x >= 0)\n",
    "con2 = (x <= 1)\n",
    "\n",
    "con3 = (x[0,:]@np.ones(5) == 1)\n",
    "con2 = (x[1,:]@np.ones(5) == 1)\n",
    "con3 = (x[2,:]@np.ones(5) == 1)\n",
    "con4 = (x[3,:]@np.ones(5) == 1)\n",
    "con5 = (x[4,:]@np.ones(5)== 1)\n",
    "\n",
    "con6 = (x[:,0]@np.ones(5) == 1)\n",
    "con7 = (x[:,1]@np.ones(5) == 1)\n",
    "con8 = (x[:,2]@np.ones(5) == 1)\n",
    "con9 = (x[:,3]@np.ones(5) == 1)\n",
    "con10 = (x[:,4]@np.ones(5)== 1)\n",
    "\n",
    "problem = cvxpy.Problem(cvxpy.Minimize(total_value), constraints=[con1,con2,con3,con4,con5,con6,con7,con8,con9,con10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.,  1., -0., -0., -0.],\n",
       "       [-0., -0., -0., -0.,  1.],\n",
       "       [ 1., -0., -0., -0., -0.],\n",
       "       [-0., -0.,  1., -0., -0.],\n",
       "       [-0., -0., -0.,  1., -0.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
