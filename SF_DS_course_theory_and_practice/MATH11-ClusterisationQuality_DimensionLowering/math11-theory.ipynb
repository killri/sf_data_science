{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества кластеризации: внутренние меры"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутренние меры - это способ оценки качества кластеризации, без правильных ответов, ориентируясь только на данные, которые мы кластеризировали\n",
    "\n",
    "В библиотеке sklearn реализованы три наиболее популярные метрики:\n",
    "\n",
    "* коэффициент силуэта (Silhouette Coefficient);\n",
    "* индекс Калински — Харабаса (Calinski-Harabasz Index);\n",
    "* индекс Дэвиса — Болдина (Davies-Bouldin Index)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### КОЭФФИЦИЕНТ СИЛУЭТА"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тот же самый коэфицент, который использовался для определения оптимального числа кластеров алгоритмах, где коэфицент надо было задавать."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэфицент силуэта = $\\overline{s} = \\frac{1}{N} \\sum_{i=1}^N \\frac{b_i-a_i}{max(a_i,b_i)}$, где $a_i$ - это среднее расстояние от $x_i$ до объектов своего кластера, а $b_i$ - это среднее расстояние от $x_i$ до объектов ближайшего другого кластера"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение коэффициента силуэта всегда находится в диапазоне [−1,1].\n",
    "\n",
    "* Значение близко к −1: объекты в кластерах разрознены, и в целом кластерную структуру не удалось выделить.\n",
    "* Значение близко к 0: кластеры пересекаются друг с другом.\n",
    "* Значение близко к 1: чёткая кластерная структура с «плотными», отделёнными друг от друга кластерами."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ИНДЕКС КАЛИНСКИ — ХАРАБАСА\n",
    "\n",
    "Показывает отношение между разбросом значений между кластерами и разбросом значений внутри кластеров и вычисляется по следующей формуле:\n",
    "\n",
    "$\\frac{SS_B}{SS_W}\\times\\frac{N-K}{K-1}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной формуле:\n",
    "\n",
    "* N — общее количество объектов;\n",
    "* K — количество кластеров;\n",
    "* $SS_B$ — взвешенная межкластерная сумма квадратов расстояний;\n",
    "* $SS_W$ — внутрикластерная сумма квадратов расстояний."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SS_B=\\sum_{k=1}^{K} n_{k} \\times\\left\\|C_{k}-C\\right\\|^{2}$\n",
    "\n",
    "В данной формуле:\n",
    "\n",
    "* $n_k$ — количество наблюдений в кластере k;\n",
    "* $C_k$ — центроид кластера k;\n",
    "* $C$ — центроид всего набора данных;\n",
    "* $K$— количество кластеров."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрикластерная сумма для кластера $k$ равна $SS_B^k = \\sum_{i=1}^{n_k}  \\times\\left\\|X_{ik}-C_k\\right\\|^{2}$\n",
    "\n",
    "В данной формуле:\n",
    "\n",
    "* $n_k$ — количество наблюдений в кластере k;\n",
    "* $C_k$ — центроид кластера k;\n",
    "* $X_{ik}$— i-ое наблюдение в кластере k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SS_B = \\sum_{k=1}^K SS_B^k$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно отметить, что нет никакого «приемлемого» порогового значения индекса — скорее, его можно использовать для того, чтобы сравнивать разные разбиения на кластеры между собой: более высокое значение индекса будет означать, что кластеры плотные (т. е. объекты внутри них находятся близко друг к другу) и хорошо разделены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#В библиотеке sklearn данный алгоритм реализуется с помощью метода calinski_harabasz_score():\n",
    "\n",
    "#определяем алгоритм кластеризации\n",
    "#km = KMeans(n_clusters=3, random_state=42)\n",
    "#обучаем его на наших данных\n",
    "#km.fit_predict(X)\n",
    "#вычисляем значение коэффициента Калински — Харабаса\n",
    "#score = calinski_harabasz_score(X, km.labels_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ИНДЕКС ДЭВИСА — БОЛДИНА (DBI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаги вычисления:\n",
    "\n",
    "1. Bычисляем для каждого кластера следующую меру разброса значений внутри него: $S_{k}=(\\frac{1}{n_{k}} \\sum_{i=1}^{n_{k}}\\left|X_{i k}-C_{k}\\right|^{q})^{\\frac{1}{q}}$\n",
    "    * $n_k$ - мощность кластера $k$\n",
    "    * $q$ - обячно = 2, и тогда расстояние Евклидово\n",
    "    * Остальное все как в предыдущем индексе\n",
    "2. Далее находим расстояния между центроидами кластеров: $M_{i, j}=\\left\\|C_{i}-C_{j}\\right\\|_{q}$\n",
    "3. Теперь для каждой пары кластеров вычисляем следующее отношение: $R_{i j}=\\frac{S_{i}+S_{j}}{M_{i j}}$. Также для каждого кластера находим максимум из полученных значений: $R_i\\equiv maximum (R_{ij})$\n",
    "4. $DBI=\\frac{1}{N}\\sum^{N}_{i=1}R_i$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индекс показывает среднюю «схожесть» между кластерами, и 0 — это минимально возможное значение. Разумеется, так как мы хотим, чтобы кластеры были максимально различными (т. е. имели низкую схожесть), мы должны пытаться достичь как можно более маленького значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#В библиотеке sklearn индекс Дэвиса — Болдина реализуется с помощью метода davies_bouldin_score():\n",
    "\n",
    "#определяем алгоритм кластеризации\n",
    "#km = KMeans(n_clusters=3, random_state=42)\n",
    "#обучаем его на наших данных\n",
    "#km.fit_predict(X)\n",
    "#вычисляем значение коэффициента Дэвиса — Болдина\n",
    "#score = davies_bouldin_score(X, km.labels_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ВНУТРИКЛАСТЕРНОЕ РАССТОЯНИЕ\n",
    "\n",
    "Для того чтобы оценить качество кластеризации, можно вычислить суммарное внутрикластерное расстояние:\n",
    "\n",
    "$F_0 = \\sum_{k=1}^{K} \\sum_{i=1}^{N}\\left[a\\left(x_{i}\\right)=k\\right] \\rho\\left(x_{i}, c_{k}\\right)$\n",
    "\n",
    "Разумеется, сумма этих расстояний должна быть минимальной — это тот случай, когда все элементы кластера совпадают с центроидом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### МЕЖКЛАСТЕРНОЕ РАССТОЯНИЕ\n",
    "\n",
    "Аналогично суммарному внутрикластерному расстоянию, вводится межкластерное расстояние:\n",
    "\n",
    "$F_0 = \\sum_{i, j=1}^{N}\\left[a\\left(x_{i}\\right) \\neq a\\left(x_{j}\\right)\\right] \\rho\\left(x_{i}, x_{j}\\right)$\n",
    "\n",
    "Мы проверяем, что предсказания о принадлежности к кластеру не равны (т. е. объекты относятся к разным кластерам), и считаем расстояние между этими объектами (можем использовать различные функции расстояний). Здесь мы, разумеется, будем максимизировать результат, так как нам важно, чтобы элементы из разных кластеров были как можно меньше похожи друг на друга, а значит, чтобы расстояние между ними было как можно больше."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ОТНОШЕНИЕ РАССТОЯНИЙ\n",
    "\n",
    "Логичным образом из предыдущих двух метрик (внутрикластерного и межкластерного расстояний) мы получаем отношение расстояний:\n",
    "\n",
    "$\\frac{F_0}{F_1}\\rightarrow \\min$\n",
    "\n",
    "Таким образом мы можем учитывать оба функционала, рассмотренные ранее (расстояние внутри кластера и между кластерами), и оптимизировать отношение расстояний. Естественно, нам нужно, чтобы оно было минимальным — это будет достигаться, если расстояние между кластерами максимально, а внутри кластера — минимально."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества кластеризации: внешние меры"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда бывает такое, что какая-то разметка в наших данных всё же есть. В этом случае мы можем использовать дополнительные показатели, которые помогут оценить качество кластеризации."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ИНДЕКС РЭНДА\n",
    "\n",
    "индекс Рэнда — это мера сходства между двумя кластеризациями. Однако, так как он позволяет сравнить два разбиения на кластеры, мы можем использовать его для сравнения фактических меток классов и прогнозируемых меток кластеров, тем самым оценивая качество наших алгоритмов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RI=\\frac{2(a+b)}{N(N-1)}$, \n",
    "\n",
    "где $a$ - число пар объектов, которые имеют одинаковые метки (т. е. в фактическом разбиении находятся в одном классе) и располагаются в одном кластере;, \n",
    "\n",
    "а $b$ - число пар объектов, которые имеют различные метки (т. е. в фактическом разбиении находятся в разных классах) и располагаются в разных кластерах"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "знаменатель здесь выражает общее количество пар, которые можно получить из нашего набора данных. То есть, по сути, в этом индексе вычисляется доля пар, для которых сохранилось их расположение по отношению друг к другу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "#Для вычисления индекса Рэнда с помощью библиотеки sklearn можно использовать метод rand_score():\n",
    "from sklearn.metrics import rand_score\n",
    "print(rand_score([1, 1, 1, 2, 2], [1, 1, 2, 2, 3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также используют скорректированный индекс Рэнда (Adjusted Rand Index): $ARI=\\frac{RI-E[RI]}{\\max(RI)-E[RI]}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его преимущество перед обычным индексом Рэнда состоит в том, что при случайных кластеризациях его значение близко к нулю вне зависимости от количества кластеров и наблюдений.\n",
    "\n",
    "Исправленный индекс Рэнда в sklearn представлен через метод adjusted_rand_score()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АЛГОРИТМЫ ПОНИЖЕНИЯ РАЗМЕРНОСТИ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis, метод главных компонент)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "АЛГОРИТМ РЕАЛИЗАЦИИ PCA\n",
    "\n",
    "1. Стандартизировать данные. (именно как векторы)\n",
    "2. Рассчитать ковариационную матрицу для объектов.\n",
    "3. Рассчитать собственные значения и собственные векторы для ковариационной матрицы.\n",
    "4. Отсортировать собственные значения и соответствующие им собственные векторы.\n",
    "5. Выбрать  наибольших собственных значений и сформировать матрицу соответствующих собственных векторов.\n",
    "6. Преобразовать исходные данные, умножив матрицу данных на матрицу отобранных собственных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "A = np.matrix([[1,2,3,4],\n",
    "               [5,5,6,7],\n",
    "               [1,4,2,3],\n",
    "               [5,3,2,1],\n",
    "               [8,1,2,2]])\n",
    "\n",
    "df = pd.DataFrame(A,columns  = ['x1','x2','x3','x4'])\n",
    "df_std  = (df - df.mean()) / (df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>1.563740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-0.173749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-1.042493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>-1.264911</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-0.608121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4\n",
       "0 -1.000000 -0.632456  0.000000  0.260623\n",
       "1  0.333333  1.264911  1.732051  1.563740\n",
       "2 -1.000000  0.632456 -0.577350 -0.173749\n",
       "3  0.333333  0.000000 -0.577350 -1.042493\n",
       "4  1.333333 -1.264911 -0.577350 -0.608121"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.31622777,  0.04811252, -0.18098843],\n",
       "       [-0.31622777,  1.        ,  0.63900965,  0.61812254],\n",
       "       [ 0.04811252,  0.63900965,  1.        ,  0.94044349],\n",
       "       [-0.18098843,  0.61812254,  0.94044349,  1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat = np.cov(df_std.T)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_val, eigen_vectors = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.51579324, 1.0652885 , 0.39388704, 0.02503121])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16195986, -0.91705888, -0.30707099,  0.19616173],\n",
       "       [-0.52404813,  0.20692161, -0.81731886,  0.12061043],\n",
       "       [-0.58589647, -0.3205394 ,  0.1882497 , -0.72009851],\n",
       "       [-0.59654663, -0.11593512,  0.44973251,  0.65454704]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.755975</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>-0.101852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.556534</td>\n",
       "      <td>-0.780432</td>\n",
       "      <td>-0.106870</td>\n",
       "      <td>-0.005757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.051480</td>\n",
       "      <td>1.253135</td>\n",
       "      <td>-0.396673</td>\n",
       "      <td>0.182141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.014150</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.679886</td>\n",
       "      <td>-0.201225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.579861</td>\n",
       "      <td>-1.228917</td>\n",
       "      <td>0.242230</td>\n",
       "      <td>0.126693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.014003  0.755975  0.941200 -0.101852\n",
       "1 -2.556534 -0.780432 -0.106870 -0.005757\n",
       "2 -0.051480  1.253135 -0.396673  0.182141\n",
       "3  1.014150  0.000239 -0.679886 -0.201225\n",
       "4  1.579861 -1.228917  0.242230  0.126693"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_std @ eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем нужный алгоритм\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "#определяем метод главных компонент с двумя компонентами\n",
    "pca = PCA(n_components=2)\n",
    "#обучаем алгоритм на наших данных\n",
    "principalComponents = pca.fit_transform(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.40033078e-02,  7.55974765e-01],\n",
       "       [ 2.55653399e+00, -7.80431775e-01],\n",
       "       [ 5.14801919e-02,  1.25313470e+00],\n",
       "       [-1.01415002e+00,  2.38808310e-04],\n",
       "       [-1.57986086e+00, -1.22891650e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.333333333333334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Задание 5.1\n",
    "\n",
    "#Найдите матрицу ковариаций для векторов (3,4,1) и (1,6,2). \n",
    "# В качестве ответа укажите сумму всех значений матрицы, округлённую до двух знаков после точки-разделителя.\n",
    "v = np.array([3,4,1])\n",
    "w = np.array([1,6,2])\n",
    "\n",
    "M = np.array([v,w])\n",
    "np.cov(M).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 5.5\n",
    "#Дана матрица признаков:\n",
    "\n",
    "A = np.matrix([[8,7,2,9],\n",
    "               [1,3,6,3],\n",
    "               [7,2,0,3],\n",
    "               [10,3,1,1],\n",
    "               [8,1,3,4]])\n",
    "#Какое минимальное количество главных компонент надо выделить, чтобы сохранить информацию о как минимум 90 % разброса данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46.1849111 , 43.41617715,  4.47619619,  5.92271556])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(A,columns  = ['x1','x2','x3','x4'])\n",
    "df_std  = (df - df.mean()) / (df.std())\n",
    "\n",
    "cov_mat = np.cov(df_std.T)\n",
    "eigen_val, eigen_vectors = np.linalg.eig(cov_mat)\n",
    "eigen_val / eigen_val.sum() * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Cингулярное разложение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с сингулярного разложения. SVD используется не только для снижения размерности, но и в целом имеет множество применений в машинном обучении, например, в рекомендательных системах — в одном из следующих модулей вы сможете изучить эту тему подробнее.\n",
    "\n",
    "Суть сингулярного разложения заключается в следующей теореме ↓\n",
    "\n",
    "Любую прямоугольную матрицу $A$ размера $n*m$ можно представить в виде произведения трёх матриц:\n",
    "\n",
    "$A_{n \\times m}=U_{n \\times n} \\cdot D_{n \\times m} \\cdot V_{m \\times m}^{T}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой формуле:\n",
    "\n",
    "* $U$ матрица размера $n*n$. Все её столбцы ортогональны друг другу и имеют единичную длину. Такие матрицы называются ортогональными. Эта матрица содержит нормированные собственные векторы матрицы $AA^T$.\n",
    "* $D$ матрица размера $n*m$. На её главной диагонали стоят числа, называемые сингулярными числами (они являются корнями из собственных значений матриц $A^TA$ и $AA^T$), а вне главной диагонали стоят нули. Если мы решаем задачу снижения размерности, то элементы этой матрицы, если их возвести в квадрат, можно интерпретировать как дисперсию, которую объясняет каждая компонента.\n",
    "* $V$ — матрица размера $m*m$. Она тоже ортогональная и содержит нормированные собственные векторы матрицы $A^TA$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы реализовать сингулярное разложение с помощью библиотеки sklearn, необходимо использовать алгоритм TruncatedSVD(), в который передаётся n_components в качестве параметра, определяющего количество итоговых компонент, которые соответвуют наибольшим сингулярным числам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём объект класса TruncatedSVD\n",
    "# n_components — размерность нового пространства, n_iter — количество итераций\n",
    "#svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "#svd.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "#transformed = svd.transform(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### t-SNE cтохастическое вложение соседей с t-распределением"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE (стохастическое вложение соседей с t-распределением). Его преимущество относительно первых двух заключается в том, что он может реализовывать уменьшение размерности и разделение для данных, которые являются линейно неразделимыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "#from sklearn.manifold import TSNE\n",
    "# создаём объект класса TSNE\n",
    "# n_components — размерность нового пространства\n",
    "#tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "# обучаем модель на данных X и применяем к матрице X уменьшение размерности\n",
    "#tsne.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
