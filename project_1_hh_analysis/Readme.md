# Проект 2. Анализ данных резюме на hh.ru

## Содержание
* Описание проекта
* Какую задачу мы решаем?
* Краткая информация о данных
* Этапы работы над проектом
* Результаты
* Выводы

## Описание проекта

Задача проекта с том что на реальных данных о вакансиях на hh отработать навыки предобработки, визуального анализа и очистки данных.

## Какую задачу мы решаем?
На старте у нас есть пакет данных о вакансиях на hh с формате .csv

По сути у нас было 3 задачи:
1. Обработать исходные данные таким, образом, чтобы они были пригодны для анализа, т.к. в исходном файле все данные были текстовые, а многие признаки сгруппированы
2. Сделать разведывательный анализ данных. Найти потенциальные выбросы, посмотреть какие параметры потенциально важны для возможных моделей, а какие нет, польщуясь средствами визуализации.
3. Очистить данные от дубликатов, пропусков и выбросов

*Оценка качества работы*

На обучающей платформе SkillFactory были размещены контрольные вопрсы по каждому шагу проекта. Это позволяло проверять правильность обработки данных на каждом этапе.

*Чему мы учимся/ что практикуем?*

* Инcтрументарий библиотеки Pandas:
    * Простейшие методы и возможности работы с DataFrame и Series
    * Статистические методы DataFrame и Series
    * Фильтры, сортировки DataFrame
    * Группировка данных и свобные таблицы
    * Слияние таблица через метод merge()
    * Применение функций для обработки DataFrame
    * Инструменты для очистки данных
* Инструменты визуализации: в данном проекте я использовал, в основном, Plotly и немного Seaborn. 
* На практике получаем понимание, как предобработать данные, чтобы с ними было удобно работать далее.
* Практикуемся в интерпретации данных: о чем говорят распределения, есть ли зависимости, что есть выбросы.

## Краткая информация о данных

Исходные данные - это таблица .csv из 44744 строк и 12 столбцов.

Все данные имеют тип object и даже числовые показатели спрятаны за формулировками типа: "Мужчина , 39 лет , родился 27 ноября 1979" или "500 USD" или "Москва, не готова к командировкам, готова к переезду" и т.п.

Ссылка на файл с данными:
https://drive.google.com/file/d/1Zd1zpF_0v66HsmwT0tUUiQcduJU1wnQO/view?usp=sharing


## Шаги работы над проектом
1. Предобработка данных
    * Преобразование строчного признака "Образование и ВУЗ" к категориальному признаку "Образование", состояшему из четких 4х категорий.
    * Преобразование признака "Пол, Возраст" в формате "Мужчина , 39 лет , родился 27 ноября 1979" к двум признакам "Пол" (категориальный из 2х категорий) и "Возраст" (числовой - целое число лет)
    * Преобразование признака "Опыт работы", состоящему из описания всех мест работы соискателя к признаку "Опыт работы (месяц)", который является просто целым числом опыта работы в месяцах
    * Преобразование признака "Город, переезд, командировки" к признакам "Город" (Категориальный из 4х категорий), "Готовность к переезду" (Булев), "Готовность к командировкам" (Булев).
    * Преобразование признаков "Занятость" и "График" в 10 булевых признаком-мигалок, для систематизации данных.
    * Преобразование признака "ЗП" к единой валюте с использованием внешних данных о курсах валют на дату обновления резюме. Тем самым мы получили признак "ЗП (руб)"
2. Разведывательный анализ данных
    * В первую очередь мы построили распределение (в виде гистограммы и коробчатой диаграммы) трех числовых признаков: "Возраст", "Опыт работы" и "ЗП (руб)".
    * Изучили зависимость ЗП от категориальных признаков "Город", "Образование", "Готовность к переезду", "Готовность к командировкам"
    * На тепловой карте посмотрели динамику изменения медианного дохода по возрасту в разных категориях образования
    * Изучили зависимость Возраста от Опыта работы на диагрмее рассеяния. Определили дополнительные выбросы.
    * Изучили, как пол влияет на уровень дохода людей с высшим образованием в разных городах.
    * Проверили имеет ли булев признак "Удаленная работа" большре влияние на числовые параметры данных "ЗП (руб)", "Возраст", "Опыт работы"
3. Очистка данных
    * Очистили данные от дублей
    * Очистили данные от пропусков
    * Очистили данные от выбросов по уровню "ЗП (Руб)"
    * Очислили данные от выбросов по "Возраст" методом 3х сигм.

## Результаты
* Все задания я сделал. Все ответы, кроме одного совпали с первой попытки (даже медианное значение ЗП после перевода в РУБ).
* Несовпавший ответ - дубликаты. У меня было меньше на 3 дубликата. Вероятно это cвязано с преобразованиями данных на первом этапе, но ошибку я не нашел, т.к. все остальные ответы у меня совпадали с первого раза. Я даже нашел в группе поддержи в Slack эталонный код на одну из функций, которую рекомендовали в поддержке проверить, если это задание не совпадает. Но число дубликатов она давала такое же, как и у меня, после примерения, а была почти вдвое длинее моей. И я оставил свой вариант. На самом деле я даже проверил дубликаты на каждом из этапов преобразования данных и как оно менялось, но ошибки нигде не увидел. В целом по этой ситуации я сделал такой вывод: **сначала нужно было удалить дубликаты, а потом заниматься преобразованием данных. А после преобразования снова можно проверить на дубликаты. Тогда можно отловить на каком этапе, возникают дополнительные дубликаты и почему - из каких полей. Это может помочь контролировать корректность всех преобразований.**
* Я привык пользоваться внешними ресурсами и научился дополнительным "фишкам" (например построение дополнительных линий на фигурах plotly.express или параметр 'marginal' объектов plotly) по ссылкам:
    * https://pandas.pydata.org/
    * https://plotly.com/graphing-libraries/

## Заключение

У меня после этого проекта, да и всего блока по pandas появилось большое желание организовать методами DataSience автозаказ на текущей работе. У меня сейчас работает, но хочется именно на python все сделать. Я уже пробовал частично, но были проблемы с кодировками т.к. данные из 1С.
И в целом, прям моя тема, я увлекался анализом и интерпретецией данных давно, но сейчас вижу, что делаю это на каком-то новом уровне.